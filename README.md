# Simple Yet Effective Fine-Tuning of Deep CNNs using an Auxiliary Classification Loss for Remote Sensing Scene Classification

The current literature of remote sensing (RS) scene classification shows that state-of-the-art results are achieved using feature extraction methods, where Convolutional Neural Networks (CNNs) (mostly VGG16 with 138.36M parameters) are used as feature extractors and then simple up to complex handcrafted modules are added for additional feature learning and classification, hence coming back to feature engineering. In this paper, we revisit the fine-tuning approach for deeper networks (GoogLeNet and Beyond) and show that it has not been well exploited due to the negative effect of the vanishing gradient problem encountered when transferring knowledge to small datasets. The aim of this work is two-fold. Firstly we provide best practices for fine-tuning pre-trained CNNs using the Root Mean Square Propagation (RMSProp) method. Secondly, we propose a simple yet effective solution for tackling the vanishing gradient problem by injecting gradients at an earlier layer of the network using an auxiliary classification loss function. Then, we fine-tune the resulting regularized network by optimizing both the primary and auxiliary losses. As for pre-trained CNNs, we consider in this work Inception-based networks and EfficientNets with small weights: GoogLeNet (7M) and EfficientNet-B0 (5.3M) and their deeper versions inception-v3 (23.83M) and EfficientNet-B3 (12M), respectively. The former have been used previously in the context of RS and yielded low accuracies compared to VGG-16, while the latter are new state-of-the-art models. Extensive experimental results on several benchmark datasets reveal clearly that if fine-tuning is done in an appropriate way it can settle new state-of-the-art results with low computational cost.
